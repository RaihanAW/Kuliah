{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Folding\n",
    "Case folding untuk menjadikan string data ke lowercase, menghilangkan angka, menghilangkan tanda baca, menghilangkan karakter kosong, dan\n",
    "memisahkan string menjadi kata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Tweet  \\\n",
      "0  Cegah mata rantai Covid-19,mari kita dirumah s...   \n",
      "1  aku mohon yaAllah semoga wabah covid-19 menghi...   \n",
      "2  Pemprov Papua Naikkan Status Jadi Tanggap Daru...   \n",
      "3            Covid belum nyampe prigen mbak hmm hoax   \n",
      "4  Nyuruh orang pintar, lu aja Togog. Itu kerumun...   \n",
      "\n",
      "                                     Processed_Tweet  \n",
      "0  cegah mata rantai covidmari kita dirumah saja ...  \n",
      "1  aku mohon yaallah semoga wabah covid menghilan...  \n",
      "2  pemprov papua naikkan status jadi tanggap daru...  \n",
      "3            covid belum nyampe prigen mbak hmm hoax  \n",
      "4  nyuruh orang pintar lu aja togog itu kerumunan...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "df = pd.read_csv('Dataset_Sentimen_Emosi.csv')\n",
    "\n",
    "#Fungsi untuk Case Folding & Split\n",
    "\n",
    "def text_preprocessing(text):\n",
    "\n",
    "    #Case Folding untuk menjadikan kalimat menjadi huruf kecil\n",
    "    text = text.lower()\n",
    "\n",
    "    #Case Folding untuk remove angka\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation + '0123456789'))\n",
    "\n",
    "    #Case Folding untuk menghilangkan karakter kosong\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    #split kalimat menjadi kata\n",
    "    split_text = text.split()\n",
    "\n",
    "    return ' '.join(split_text)\n",
    "\n",
    "#Preprocessing Dataset Sentimen\n",
    "df['Processed_Tweet'] = df['Tweet'].apply(text_preprocessing)\n",
    "\n",
    "#Save Data baru ke CSV\n",
    "df.to_csv('Case Folding dan Split/case_folding_split_data.csv', index=False)\n",
    "\n",
    "print(df[['Tweet', 'Processed_Tweet']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Code untuk mengubah kalimat kalimat yang telah diproses sebelumnya menjadi token token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Processed_Tweet</th>\n",
       "      <th>Tokenized_Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cegah mata rantai covidmari kita dirumah saja ...</td>\n",
       "      <td>[cegah, mata, rantai, covidmari, kita, dirumah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aku mohon yaallah semoga wabah covid menghilan...</td>\n",
       "      <td>[aku, mohon, yaallah, semoga, wabah, covid, me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pemprov papua naikkan status jadi tanggap daru...</td>\n",
       "      <td>[pemprov, papua, naikkan, status, jadi, tangga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>covid belum nyampe prigen mbak hmm hoax</td>\n",
       "      <td>[covid, belum, nyampe, prigen, mbak, hmm, hoax]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nyuruh orang pintar lu aja togog itu kerumunan...</td>\n",
       "      <td>[nyuruh, orang, pintar, lu, aja, togog, itu, k...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Processed_Tweet  \\\n",
       "0  cegah mata rantai covidmari kita dirumah saja ...   \n",
       "1  aku mohon yaallah semoga wabah covid menghilan...   \n",
       "2  pemprov papua naikkan status jadi tanggap daru...   \n",
       "3            covid belum nyampe prigen mbak hmm hoax   \n",
       "4  nyuruh orang pintar lu aja togog itu kerumunan...   \n",
       "\n",
       "                                     Tokenized_Tweet  \n",
       "0  [cegah, mata, rantai, covidmari, kita, dirumah...  \n",
       "1  [aku, mohon, yaallah, semoga, wabah, covid, me...  \n",
       "2  [pemprov, papua, naikkan, status, jadi, tangga...  \n",
       "3    [covid, belum, nyampe, prigen, mbak, hmm, hoax]  \n",
       "4  [nyuruh, orang, pintar, lu, aja, togog, itu, k...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split data yang sudah diproses tadi menjadi token token\n",
    "df['Tokenized_Tweet'] = df['Processed_Tweet'].apply(lambda x: x.split())\n",
    "\n",
    "#Save perbandingan data ke file CSV Baru\n",
    "df[['Processed_Tweet', 'Tokenized_Tweet']].to_csv('Tokenized Data/clean_tokenized_tweet_data.csv', index=False)\n",
    "\n",
    "#Tampilan beberapa baris untuk evaluasi proses tokenization\n",
    "df[['Processed_Tweet', 'Tokenized_Tweet']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Misspell Correction\n",
    "Code untuk mengubah Typo pada token menggunakan kamus kata yang diperbaiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokenized_Tweet</th>\n",
       "      <th>Corrected_Token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[cegah, mata, rantai, covidmari, kita, dirumah...</td>\n",
       "      <td>[cegah, mata, rantai, covidmari, kita, di, rum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[aku, mohon, yaallah, semoga, wabah, covid, me...</td>\n",
       "      <td>[aku, mohon, ya, allah, semoga, wabah, covid, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[pemprov, papua, naikkan, status, jadi, tangga...</td>\n",
       "      <td>[pemerintah, provinsi, papua, naikan, status, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[covid, belum, nyampe, prigen, mbak, hmm, hoax]</td>\n",
       "      <td>[covid, belum, sampai, prigen, mbak, hem, hoaks]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[nyuruh, orang, pintar, lu, aja, togog, itu, k...</td>\n",
       "      <td>[menyuruh, orang, pintar, lu, aja, togog, itu,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Tokenized_Tweet  \\\n",
       "0  [cegah, mata, rantai, covidmari, kita, dirumah...   \n",
       "1  [aku, mohon, yaallah, semoga, wabah, covid, me...   \n",
       "2  [pemprov, papua, naikkan, status, jadi, tangga...   \n",
       "3    [covid, belum, nyampe, prigen, mbak, hmm, hoax]   \n",
       "4  [nyuruh, orang, pintar, lu, aja, togog, itu, k...   \n",
       "\n",
       "                                     Corrected_Token  \n",
       "0  [cegah, mata, rantai, covidmari, kita, di, rum...  \n",
       "1  [aku, mohon, ya, allah, semoga, wabah, covid, ...  \n",
       "2  [pemerintah, provinsi, papua, naikan, status, ...  \n",
       "3   [covid, belum, sampai, prigen, mbak, hem, hoaks]  \n",
       "4  [menyuruh, orang, pintar, lu, aja, togog, itu,...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Read data kamus untuk membenahi token\n",
    "dictionary_df = pd.read_csv('kamus_clean.csv')\n",
    "\n",
    "dictionary = dict(zip(dictionary_df['TIDAK BAKU'], dictionary_df['BAKU']))\n",
    "\n",
    "#Fungsi untuk membenahi token\n",
    "def check_token(tokens, dictionary):\n",
    "    corrected_token = [dictionary.get(token, token) for token in tokens]\n",
    "    return corrected_token\n",
    "\n",
    "#Block Kode untuk Membenahi Token\n",
    "df['Corrected_Token'] = df['Tokenized_Tweet'].apply(lambda tokens: check_token(tokens, dictionary))\n",
    "df['Corrected_Token'] = df['Corrected_Token'].apply(lambda tokens: ' '.join(tokens))\n",
    "df['Corrected_Token'] = df['Corrected_Token'].apply(lambda x: x.split())\n",
    "\n",
    "#Save Token ke CSV untuk Token yang sudah dipreprocess\n",
    "df[['Tokenized_Tweet', 'Corrected_Token']].to_csv('Clean Token/clean_token.csv', index=False)\n",
    "\n",
    "#Print beberapa sample\n",
    "df[['Tokenized_Tweet', 'Corrected_Token']].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
